{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc6de5c3-9a20-444f-92e2-96a72a57f075",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%pip install -U llama-index llama-index-llms-databricks mlflow fhir.resources\n",
    "#%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e36d0a65-16b5-4d66-88cc-73b17ecc88fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.databricks import Databricks\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import random\n",
    "import json\n",
    "from pprint import pprint\n",
    "##  import #pprint\n",
    "from library import get_all_fhir_resource_info_with_descriptions\n",
    "import json\n",
    "##  import #pprint\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "# Assuming 'llm' is already initialized (e.g., llm = Databricks(endpoint_name=\"...\") )\n",
    "# You would import your function like this:\n",
    "from library import get_all_fhir_resource_info_with_descriptions\n",
    "import json\n",
    "##  import #pprint\n",
    "\n",
    "# Import specific FHIR resource classes from fhir.resources library\n",
    "# Ensure you have installed it: pip install fhir.resources\n",
    "from fhir.resources.location import Location\n",
    "from fhir.resources.organization import Organization\n",
    "from fhir.resources.address import Address\n",
    "from fhir.resources.codeableconcept import CodeableConcept\n",
    "from fhir.resources.coding import Coding\n",
    "\n",
    "\n",
    "w = WorkspaceClient()\n",
    "tmp_token = w.tokens.create(comment=\"for model serving\", lifetime_seconds=14400)\n",
    "\n",
    "\n",
    "llm = Databricks(\n",
    "    model=\"databricks-llama-4-maverick\",\n",
    "    api_key=tmp_token.token_value,\n",
    "    api_base=f\"{w.config.host}/serving-endpoints/\"\n",
    ")\n",
    "\n",
    "# Completion\n",
    "#llm.complete(\"Hello, world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bd0939a-abae-4b98-a9ab-0d49ef63207b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>api_information_source_name</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated</th>\n",
       "      <th>list_source</th>\n",
       "      <th>certified_api_developer_name</th>\n",
       "      <th>capability_fhir_version</th>\n",
       "      <th>format</th>\n",
       "      <th>http_response</th>\n",
       "      <th>http_response_time_second</th>\n",
       "      <th>smart_http_response</th>\n",
       "      <th>errors</th>\n",
       "      <th>cap_stat_exists</th>\n",
       "      <th>kind</th>\n",
       "      <th>requested_fhir_version</th>\n",
       "      <th>is_chpl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>https://fhir-usa.unify.chbase.com/org/wyoming-...</td>\n",
       "      <td>WYOMING COUNTY COMMUNITY</td>\n",
       "      <td>24/5/2025 6:40</td>\n",
       "      <td>9/6/2025 8:24</td>\n",
       "      <td>https://fhir-usa.unify.chbase.com/.well-known/...</td>\n",
       "      <td>TruBridge, Inc.</td>\n",
       "      <td>4.0.1</td>\n",
       "      <td>application/fhir+json,application/fhir+xml</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.3144</td>\n",
       "      <td>200.0</td>\n",
       "      <td>None</td>\n",
       "      <td>true*</td>\n",
       "      <td>capability</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1322</th>\n",
       "      <td>https://haiku.sparrow.org/fhir-prd/api/FHIR/R4/</td>\n",
       "      <td>University of Michigan Health-Sparrow</td>\n",
       "      <td>10/5/2025 23:35</td>\n",
       "      <td>9/6/2025 3:16</td>\n",
       "      <td>https://open.epic.com/Endpoints/Brands</td>\n",
       "      <td>Epic Systems Corporation</td>\n",
       "      <td>4.0.1</td>\n",
       "      <td>xml,json</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.1819</td>\n",
       "      <td>200.0</td>\n",
       "      <td>None</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>instance</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>https://api.platform.athenahealth.com/21519/br...</td>\n",
       "      <td>Texas Online Primary Care</td>\n",
       "      <td>24/5/2025 13:26</td>\n",
       "      <td>9/6/2025 3:25</td>\n",
       "      <td>https://service-base-urls.api.fhir.athena.io/a...</td>\n",
       "      <td>athenahealth, Inc.</td>\n",
       "      <td>4.0.1</td>\n",
       "      <td>json,application/json,application/json+fhir,ap...</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0862</td>\n",
       "      <td>200.0</td>\n",
       "      <td>None</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>instance</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>https://api.platform.athenahealth.com/21499/br...</td>\n",
       "      <td>Texas Gynecology</td>\n",
       "      <td>24/5/2025 5:53</td>\n",
       "      <td>8/6/2025 23:52</td>\n",
       "      <td>https://service-base-urls.api.fhir.athena.io/a...</td>\n",
       "      <td>athenahealth, Inc.</td>\n",
       "      <td>4.0.1</td>\n",
       "      <td>json,application/json,application/json+fhir,ap...</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0892</td>\n",
       "      <td>200.0</td>\n",
       "      <td>None</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>instance</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>https://fhir-myrecord.cerner.com/r4/329afbd4-f...</td>\n",
       "      <td>New York Comprehensive Care P.C.</td>\n",
       "      <td>11/5/2025 8:34</td>\n",
       "      <td>9/6/2025 4:58</td>\n",
       "      <td>https://raw.githubusercontent.com/oracle-sampl...</td>\n",
       "      <td>Cerner Corporation</td>\n",
       "      <td>4.0.1</td>\n",
       "      <td>json,application/fhir+json</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.5025</td>\n",
       "      <td>200.0</td>\n",
       "      <td>None</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>instance</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    url  ... is_chpl\n",
       "540   https://fhir-usa.unify.chbase.com/org/wyoming-...  ...    True\n",
       "1322    https://haiku.sparrow.org/fhir-prd/api/FHIR/R4/  ...    True\n",
       "1725  https://api.platform.athenahealth.com/21519/br...  ...    True\n",
       "1069  https://api.platform.athenahealth.com/21499/br...  ...    True\n",
       "1112  https://fhir-myrecord.cerner.com/r4/329afbd4-f...  ...    True\n",
       "\n",
       "[5 rows x 16 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Configuration (can be moved to a config file in a real project) ---\n",
    "endpoint_loc = \"workspace.default.fhir_endpoints\"\n",
    "us_states = [\"alabama\", \"alaska\", \"arizona\", \"arkansas\", \"california\", \"colorado\", \"connecticut\", \"delaware\",\n",
    "             \"florida\", \"georgia\", \"hawaii\", \"idaho\", \"illinois\", \"indiana\", \"iowa\", \"kansas\", \"kentucky\",\n",
    "             \"louisiana\", \"maine\", \"maryland\", \"massachusetts\", \"michigan\", \"minnesota\", \"mississippi\",\n",
    "             \"missouri\", \"montana\", \"nebraska\", \"nevada\", \"new hampshire\", \"new jersey\", \"new mexico\",\n",
    "             \"new york\", \"north carolina\", \"north dakota\", \"ohio\", \"oklahoma\", \"oregon\", \"pennsylvania\",\n",
    "             \"rhode island\", \"south carolina\", \"south dakota\", \"tennessee\", \"texas\", \"utah\", \"vermont\",\n",
    "             \"virginia\", \"washington\", \"west virginia\", \"wisconsin\", \"wyoming\"]\n",
    "\n",
    "target_column = \"api_information_source_name\" # Column containing the unstructured text\n",
    "\n",
    "# --- Step 1: Get data and prepare for LLM ---\n",
    "# Load data from the specified endpoint table\n",
    "df_ep = spark.table(endpoint_loc).filter(\n",
    "    (F.col(target_column).isNotNull()) &\n",
    "    (F.col(target_column) != '200') & # Filter out non-meaningful entries\n",
    "    (F.lower(F.col(target_column)).rlike('|'.join(us_states))) # Ensure it contains a US state\n",
    ")\n",
    "\n",
    "# Convert to Pandas DataFrame for easier sampling (for demonstration)\n",
    "# For larger datasets, consider using Spark's sample or UDFs for LLM calls directly\n",
    "df_ep_pd = df_ep.toPandas()\n",
    "df_ep_pd.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdcf6c4d-be5a-441f-97db-1b7bb2a8d657",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d08ce606-e09e-4eb3-8f79-fbebb5e7661e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Processing the following unstructured text:** 'WESTERN ARIZONA REGIONAL MEDICAL CENTER'\n\nProcessing For FHIR TABULATION: \n{'fhir_resource_type': 'Organization',\n 'reasoning': 'The provided data includes a name and address details typical '\n              'of a medical institution, best represented as a FHIR '\n              'Organization resource.',\n 'relevant_data': {'address': [{'city': '',\n                                'country': 'USA',\n                                'state': 'Arizona'}],\n                   'identifier': None,\n                   'name': 'WESTERN ARIZONA REGIONAL MEDICAL CENTER',\n                   'telecom': None}}\n\n--- Summary of All Generated FHIR HL7 JSON Outputs ---\n\nResource 1:\n{'active': None,\n 'address': [{'city': '', 'country': 'USA', 'state': 'Arizona'}],\n 'alias': [],\n 'contact': [],\n 'endpoint': [],\n 'identifier': [],\n 'name': 'WESTERN ARIZONA REGIONAL MEDICAL CENTER',\n 'partOf': None,\n 'qualification': [],\n 'resourceType': 'Organization',\n 'telecom': [],\n 'type': []}\n------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Select a random entry for processing\n",
    "if not df_ep_pd.empty:\n",
    "    random_entry_name = df_ep_pd[target_column].sample(n=1).iloc[0]\n",
    "else:\n",
    "    random_entry_name = \"No matching entries found in the table.\" # Handle empty DataFrame case\n",
    "\n",
    "\n",
    "print(f\"**Processing the following unstructured text:** '{random_entry_name}'\\n\")\n",
    "\n",
    "# Initialize the LLM (ensure your Databricks LLM is configured)\n",
    "# This assumes 'llm' is already initialized as a Databricks LLM instance as in your original code.\n",
    "# For example: llm = Databricks(endpoint_name=\"databricks-mixtral-8x7b-instruct\") # or your specific endpoint\n",
    "# Make sure your Databricks WorkspaceClient is configured for authentication.\n",
    "# w = WorkspaceClient() # this usually handles authentication if you're running in a Databricks notebook\n",
    "\n",
    "# --- System Message: Generalizable for any FHIR resource (that the LLM knows) ---\n",
    "# We make the system message flexible enough for the LLM to choose the best FHIR resource type\n",
    "# and extract relevant fields based on the input text.\n",
    "system_message = (\n",
    "    \"You are an expert at extracting FHIR data from unstructured healthcare text. \"\n",
    "    \"Your goal is to parse the provided text and identify information relevant to a FHIR resource. \"\n",
    "    \"You should determine the most appropriate FHIR resource type (e.g., Location, Practitioner, Organization, Patient) \"\n",
    "    \"based on the input text. \"\n",
    "    \"Then, extract key fields for that resource, such as 'name', 'address' (with 'city', 'state', 'country'), 'identifier', 'telecom', etc. \"\n",
    "    \"For 'address', always include 'city', 'state', and 'country'. \"\n",
    "    \"Infer and guess data based on common knowledge (e.g., 'California' for 'Los Angeles', 'USA' for US states). \"\n",
    "    \"Return the extracted information as a JSON array, where each element is a FHIR-compliant JSON object. \"\n",
    "    \"Use empty strings or nulls for fields if the information is not present or cannot be reasonably inferred.\"\n",
    ")\n",
    "\n",
    "# --- User Prompt: Focus on the specific extraction task ---\n",
    "# The prompt is simplified to just ask for the extraction based on the input text.\n",
    "user_prompt = f\"Extract FHIR resource information from this text:\\n'{random_entry_name}'\"\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=system_message),\n",
    "    ChatMessage(role=\"user\", content=user_prompt),\n",
    "]\n",
    "\n",
    "# Assuming 'llm' is already defined and configured from your environment\n",
    "output = llm.chat(messages)\n",
    "\n",
    "# --- Process LLM Output (assuming JSON response) ---\n",
    "# Your existing code for parsing the LLM's response\n",
    "try:\n",
    "    # LLMs sometimes wrap JSON in markdown blocks, so we need to extract it\n",
    "    raw_llm_output = output.message.content\n",
    "    # Simple heuristic to find the JSON block\n",
    "    if '```json' in raw_llm_output and '```' in raw_llm_output:\n",
    "        json_start = raw_llm_output.find('```json') + len('```json')\n",
    "        json_end = raw_llm_output.rfind('```')\n",
    "        json_string = raw_llm_output[json_start:json_end].strip()\n",
    "    else:\n",
    "        json_string = raw_llm_output.strip() # Assume it's just JSON if no markdown\n",
    "\n",
    "    extracted_data_step1 = json.loads(json_string)\n",
    "    #print(\"\\n--- LLM's Raw Output (Extracted JSON) ---\")\n",
    "    #pprint(extracted_data_step1)\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    print(f\"\\nAn unexpected error occurred: {e}\")\n",
    "    print(\"\\n--- Raw LLM Output (For Debugging) ---\")\n",
    "    print(output.message.content)\n",
    "# --- Assume 'extracted_data_step1' is available from Step 1's execution ---\n",
    "# For demonstration in this isolated block, we'll use a sample.\n",
    "# In your actual pipeline, this variable would be populated by the previous step.\n",
    "#extracted_data_step1 = extracted_data\n",
    "\n",
    "#example\n",
    "# # [\n",
    "#   {\n",
    "#     \"resourceType\": \"Organization\", # This would come from LLM's initial extraction in Step 1\n",
    "#     \"name\": \"Nevada Medical and Pain Institute\",\n",
    "#     \"address\": {\n",
    "#       \"city\": \"\", # Example of a missing city\n",
    "#       \"state\": \"Nevada\",\n",
    "#       \"country\": \"USA\"\n",
    "#     },\n",
    "#     \"identifier\": None,\n",
    "#     \"telecom\": None\n",
    "#   }\n",
    "# ]\n",
    "\n",
    "# --- Build FHIR Context for the LLM ---\n",
    "\n",
    "# Get the structured FHIR resource descriptions dynamically using your imported function\n",
    "#fhir_resource_descriptions = get_all_fhir_resource_info_with_descriptions()\n",
    "\n",
    "# Define the full table name\n",
    "table_name = \"workspace.default.fhir_resource_descriptions\"\n",
    "\n",
    "# 1. Read the data from the Delta table into a Spark DataFrame\n",
    "#print(f\"Reading data from table: `{table_name}`...\")\n",
    "fhir_df = spark.read.format(\"delta\").table(table_name)\n",
    "\n",
    "# 2. Convert the Spark DataFrame to a Pandas DataFrame\n",
    "# This is useful if you want to perform local Python operations\n",
    "# or work with the data outside of Spark's distributed environment.\n",
    "#print(\"\\nConverting Spark DataFrame to Pandas DataFrame...\")\n",
    "fhir_pandas_df = fhir_df.toPandas()\n",
    "\n",
    "# 3. Convert the Pandas DataFrame to a list of dictionaries\n",
    "# Each dictionary will represent a row in your table.\n",
    "#print(\"\\nConverting Pandas DataFrame to list of dictionaries...\")\n",
    "fhir_resource_descriptions = fhir_pandas_df.to_dict(orient='records')\n",
    "\n",
    "# Format these descriptions into a readable string for the LLM's prompt\n",
    "fhir_context_string = (\n",
    "    \"Refer to the following list of FHIR Resource Types and their short descriptions \"\n",
    "    \"to help you categorize the extracted information. Pay attention to the purpose \"\n",
    "    \"of each resource to make the best classification:\\n\\n\"\n",
    ")\n",
    "for res_info in fhir_resource_descriptions:\n",
    "    fhir_context_string += (\n",
    "        f\"- **{res_info['fhir_resource_name']}**: {res_info['fhir_resource_description']}\\n\"\n",
    "    )\n",
    "fhir_context_string += \"\\n\"\n",
    "\n",
    "# --- Define LLM System Message and User Prompt ---\n",
    "\n",
    "system_message_step2_final = (\n",
    "    \"You are an expert AI assistant specializing in analyzing structured healthcare data. \"\n",
    "    \"Your primary goal is to review the provided JSON data (extracted from an unstructured source) \"\n",
    "    \"and categorize its content into meaningful, FHIR-relevant buckets. \"\n",
    "    \"You MUST use the provided list of FHIR Resource Types and their descriptions as a precise guide \"\n",
    "    \"to identify the most appropriate FHIR resource type(s) for the extracted information. \"\n",
    "    \"For each categorized piece of information, describe what it represents, its value, \"\n",
    "    \"and explicitly suggest the most suitable FHIR resource type (e.g., 'Organization', 'Location', 'Practitioner', etc.) \"\n",
    "    \"and specific fields that would hold this information. \"\n",
    "    \"Also, infer any potential relationships or 'edges' to other FHIR resources \"\n",
    "    \"(e.g., an 'Organization' operates at a 'Location'). \"\n",
    "    \"Do NOT generate JSON output. Instead, provide a clear, natural language summary or a bulleted list of your findings.\"\n",
    ")\n",
    "\n",
    "# Combine the extracted data and the dynamically generated FHIR context for the user prompt\n",
    "if 'extracted_data_step1' in locals() and extracted_data_step1:\n",
    "    user_prompt_step2_final = (\n",
    "        \"Here is the extracted information from Step 1:\\n\"\n",
    "        f\"```json\\n{json.dumps(extracted_data_step1, indent=2)}\\n```\\n\\n\"\n",
    "        f\"{fhir_context_string}\" # Insert the dynamically generated FHIR context\n",
    "        \"Please analyze the extracted information and categorize it into meaningful buckets, \"\n",
    "        \"explicitly mapping it to the most relevant FHIR resource types and their key fields from the examples. \"\n",
    "        \"Also, note any potential relationships or 'edges' between these categorized pieces of information \"\n",
    "        \"that would be useful for a FHIR database.\"\n",
    "    )\n",
    "else:\n",
    "    user_prompt_step2_final = \"No structured data was extracted in Step 1 to analyze for categorization.\"\n",
    "\n",
    "# --- Construct and Execute LLM Chat ---\n",
    "\n",
    "messages_step2_final = [\n",
    "    ChatMessage(role=\"system\", content=system_message_step2_final),\n",
    "    ChatMessage(role=\"user\", content=user_prompt_step2_final),\n",
    "]\n",
    "\n",
    "#print(f\"\\n**--- Step 2: LLM's Stream of Consciousness with Dynamic FHIR Context ---**\")\n",
    "\n",
    "# Call the LLM with the new messages for Step 2\n",
    "# Remember to have your 'llm' object initialized before running this part.\n",
    "# Example: llm = Databricks(endpoint_name=\"databricks-mixtral-8x7b-instruct\")\n",
    "# output_step2_final = llm.chat(messages_step2_final)\n",
    "# #print(output_step2_final.message.content)\n",
    "\n",
    "# For demonstration without an active LLM connection, we'll #print the full prompt:\n",
    "#print(\"\\n--- Full User Prompt for LLM (for review) ---\")\n",
    "#print(user_prompt_step2_final)\n",
    "\n",
    "\n",
    "# Call the LLM with the messages for Step 2\n",
    "output_step2 = llm.chat(messages_step2_final)\n",
    "\n",
    "# #print the raw content for \"stream of consciousness\"\n",
    "#print(output_step2.message.content)\n",
    "\n",
    "# Assuming 'llm' is already initialized (e.g., llm = Databricks(endpoint_name=\"...\") )\n",
    "\n",
    "# --- Assume inputs from previous steps are available ---\n",
    "# extracted_data_step1: The raw structured JSON from Step 1 (e.g., list of dicts)\n",
    "# output_step2: The ChatCompletionResponse object from Step 2, containing the natural language summary.\n",
    "\n",
    "# For demonstration purposes, let's create sample inputs:\n",
    "\n",
    "\n",
    "# Simulate the output from Step 2 (natural language stream of consciousness)\n",
    "# In your actual pipeline, this would be output_step2.message.content\n",
    "sample_output_step2_content = \"\"\"\n",
    "Based on the extracted information:\n",
    "\n",
    "-   **Nevada Medical and Pain Institute**: This strongly suggests an **Organization** resource.\n",
    "    -   **Name**: \"Nevada Medical and Pain Institute\" (maps to `Organization.name`)\n",
    "    -   **Address**: Contains 'Nevada' (State, maps to `Organization.address.state`) and 'USA' (Country, maps to `Organization.address.country`). The city is not explicitly provided, which aligns with `Organization.address.city` being potentially absent.\n",
    "    -   **Identifier**: Not present (maps to `Organization.identifier` as null).\n",
    "    -   **Telecom**: Not present (maps to `Organization.telecom` as null).\n",
    "    -   **Reasoning**: The name clearly indicates a medical institution, which is best represented as a FHIR Organization.\n",
    "    -   **Potential Edge**: This Organization would likely be associated with one or more **Location** resources.\n",
    "\"\"\"\n",
    "\n",
    "# --- Step 3: Extract FHIR Resource, Reasoning, and Relevant Data ---\n",
    "\n",
    "system_message_step3 = (\n",
    "    \"You are an expert AI assistant specialized in converting categorized healthcare data \"\n",
    "    \"into a structured JSON format that directly informs FHIR resource creation. \"\n",
    "    \"You will be given two inputs: \"\n",
    "    \"1. The initial JSON data extracted from an unstructured source (Step 1 output). \"\n",
    "    \"2. A natural language summary/categorization of that data, informed by FHIR schemas (Step 2 output). \"\n",
    "    \"Your task is to identify each major FHIR resource suggested in the Step 2 output \"\n",
    "    \"(ignoring inferred relationships/edges for now) and provide the following for each: \"\n",
    "    \"- **`fhir_resource_type`**: This key **MUST** contain the specific FHIR resource type \"\n",
    "    \" (e.g., 'Organization', 'Location', 'Practitioner') identified by Step 2. \"\n",
    "    \"- **`relevant_data`**: A JSON object containing the key-value pairs of data from the Step 1 output that are relevant to this FHIR resource. Only include fields that directly map to standard FHIR properties of this resource type. If a field from Step 1's output is not a direct FHIR property (e.g., a custom field), omit it. \"\n",
    "    \"- **`reasoning`**: A concise explanation (1-2 sentences) of why this resource type is chosen and how the data maps. \"\n",
    "    \"**CRITICAL**: Your output MUST be ONLY a JSON array, with absolutely no other text, commentary, \"\n",
    "    \"markdown fences (```json), or other characters before or after the JSON. \"\n",
    "    \"The JSON should be structured as: `[{'fhir_resource_type': '...', 'relevant_data': {...}, 'reasoning': '...'}]`.\"\n",
    ")\n",
    "\n",
    "# Combine inputs for the LLM's user prompt\n",
    "user_prompt_step3 = (\n",
    "    \"Here is the initial extracted JSON data from Step 1:\\n\"\n",
    "    f\"```json\\n{json.dumps(extracted_data_step1, indent=2)}\\n```\\n\\n\"\n",
    "    \"Here is the natural language categorization and reasoning from Step 2:\\n\"\n",
    "    f\"```\\n{sample_output_step2_content}\\n```\\n\\n\"\n",
    "    \"Please extract the core FHIR resources from these inputs, providing the resource type, \"\n",
    "    \"the relevant data mapped from Step 1's JSON, and the reasoning for each, in a JSON array format. \"\n",
    "    \"Remember, your response MUST be ONLY the JSON, and the resource type key MUST be `fhir_resource_type`.\"\n",
    ")\n",
    "\n",
    "messages_step3 = [\n",
    "    ChatMessage(role=\"system\", content=system_message_step3),\n",
    "    ChatMessage(role=\"user\", content=user_prompt_step3),\n",
    "]\n",
    "\n",
    "#print(f\"\\n**--- Step 3: LLM Extracting FHIR Resource, Reasoning, and Relevant Data ---**\")\n",
    "\n",
    "# Call the LLM with the new messages for Step 3\n",
    "output_step3 = llm.chat(messages_step3)\n",
    "raw_llm_output_step3 = output_step3.message.content\n",
    "\n",
    "# For demonstration without an active LLM connection, let's use a simulated LLM output\n",
    "# that reflects the desired pure JSON.\n",
    "\n",
    "\n",
    "# --- Robust JSON Parsing Logic ---\n",
    "extracted_resources_for_step4 = None\n",
    "try:\n",
    "    # 1. Try to find JSON within markdown fences (most reliable if LLM uses them)\n",
    "    json_string = \"\"\n",
    "    if '```json' in raw_llm_output_step3 and '```' in raw_llm_output_step3:\n",
    "        json_start = raw_llm_output_step3.find('```json') + len('```json')\n",
    "        json_end = raw_llm_output_step3.rfind('```')\n",
    "        json_string = raw_llm_output_step3[json_start:json_end].strip()\n",
    "    else:\n",
    "        # 2. Fallback: Find the first '[' and the last ']' for an array, or '{' and '}' for an object\n",
    "        # This handles cases where LLM might omit markdown but output clean JSON\n",
    "        first_char = raw_llm_output_step3.find('[')\n",
    "        last_char = raw_llm_output_step3.rfind(']')\n",
    "\n",
    "        if first_char == -1 or last_char == -1 or last_char < first_char:\n",
    "            # Fallback for single object if not an array\n",
    "            first_char = raw_llm_output_step3.find('{')\n",
    "            last_char = raw_llm_output_step3.rfind('}')\n",
    "\n",
    "        if first_char != -1 and last_char != -1 and last_char > first_char:\n",
    "            json_string = raw_llm_output_step3[first_char : last_char + 1].strip()\n",
    "        else:\n",
    "            json_string = raw_llm_output_step3.strip() # As a last resort, try parsing the whole thing\n",
    "\n",
    "    extracted_resources_for_step4 = json.loads(json_string)\n",
    "    #print(\"\\n--- Step 3 LLM's Clean JSON Output (for Step 4) ---\")\n",
    "    #pprint(extracted_resources_for_step4)\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred during parsing: {e}\")\n",
    "    print(f\"Full raw LLM output:\\n---\\n{raw_llm_output_step3}\\n---\")\n",
    "\n",
    "\n",
    "# This 'extracted_resources_for_step4' is the list of dictionaries\n",
    "# that will be the input for your next step (Step 4: building FHIR Entry).\n",
    "\n",
    "\n",
    "# Assuming 'llm' is already initialized (e.g., llm = Databricks(endpoint_name=\"...\") )\n",
    "# Assuming 'spark' is initialized for schema_df lookup\n",
    "\n",
    "# --- Inputs from previous steps (assumed to be available) ---\n",
    "# extracted_resources_for_step4: List of dictionaries from Step 3's output.\n",
    "#   Each dict should contain 'fhir_resource_type', 'relevant_data', and 'reasoning'.\n",
    "# schema_df: Pandas DataFrame loaded from \"workspace.default.fhir_definitions_table_2\".\n",
    "#   It should have 'name' and 'schema' columns.\n",
    "\n",
    "final_fhir_json_outputs = []\n",
    "\n",
    "#print(\"\\n--- Step 4: Generating FHIR HL7 JSONs via LLM ---\")\n",
    "\n",
    "#!!!\n",
    "schema_df = spark.table(\"workspace.default.fhir_definitions_table_2\").toPandas()\n",
    "\n",
    "# Iterate through each resource identified in Step 3's output\n",
    "for resource_item in extracted_resources_for_step4:\n",
    "    print(\"Processing For FHIR TABULATION: \")\n",
    "    pprint(resource_item)\n",
    "    fhir_type_str = resource_item[\"fhir_resource_type\"]\n",
    "    relevant_data = json.dumps(resource_item)\n",
    "    reasoning_from_step3 = json.dumps(resource_item)\n",
    "\n",
    "    #print(f\"\\nProcessing to generate FHIR '{fhir_type_str}' resource...\")\n",
    "\n",
    "    # Retrieve the specific schema for the resource type from schema_df\n",
    "    matching_schema_row = schema_df[schema_df['name'].str.lower() == fhir_type_str.lower()]\n",
    "    \n",
    "    if matching_schema_row.empty:\n",
    "        #print(f\"WARNING: No schema found in schema_df for resource type '{fhir_type_str}'. Cannot provide schema context for LLM. Skipping this resource.\")\n",
    "        continue # Skip to the next resource if schema is not found\n",
    "\n",
    "    retrieved_schema_value = matching_schema_row['schema'].iloc[0]\n",
    "\n",
    "    # --- LLM Prompt for Step 4 ---\n",
    "    system_message_step4 = (\n",
    "        \"You are an expert AI assistant specialized in generating FHIR HL7 compliant JSON. \"\n",
    "        \"Your task is to produce a single FHIR resource JSON object based on the provided data and schema. \"\n",
    "        \"Adhere strictly to the given FHIR JSON Schema. \"\n",
    "        \"Ensure all data from 'relevant_data' is mapped to the correct FHIR fields. \"\n",
    "        \"If a field from 'relevant_data' maps to a FHIR property that can accept null or an empty string/array, \"\n",
    "        \"and the value is missing or empty, it is acceptable to include it as `null` or `[]` (empty array/object) if appropriate for the FHIR type. \"\n",
    "        \"Do NOT include any extra fields that are not part of the FHIR schema. \"\n",
    "        \"**CRITICAL**: Your output MUST be ONLY a single FHIR JSON object, with absolutely no other text, \"\n",
    "        \"commentary, markdown fences (```json), or other characters before or after the JSON.\"\n",
    "    )\n",
    "\n",
    "    user_prompt_step4 = (\n",
    "        f\"Generate a FHIR {fhir_type_str} resource in JSON format.\\n\\n\"\n",
    "        \"Here is the data relevant to this resource:\\n\"\n",
    "        f\"```json\\n{json.dumps(relevant_data, indent=2)}\\n```\\n\\n\"\n",
    "        \"Here is the FHIR JSON Schema for a \"\n",
    "        f\"{fhir_type_str} resource. Use this schema strictly for structure and data types:\\n\"\n",
    "        f\"```json\\n{json.dumps(json.loads(retrieved_schema_value), indent=2)}\\n```\\n\\n\"\n",
    "        \"Please provide ONLY the FHIR JSON. Remember that nulls and empty strings/arrays are acceptable for missing data where allowed by FHIR.\"\n",
    "    )\n",
    "\n",
    "    messages_step4 = [\n",
    "        ChatMessage(role=\"system\", content=system_message_step4),\n",
    "        ChatMessage(role=\"user\", content=user_prompt_step4),\n",
    "    ]\n",
    "\n",
    "    #print(f\"  Requesting LLM to generate FHIR '{fhir_type_str}' JSON...\")\n",
    "    \n",
    "    # --- Call the LLM ---\n",
    "    output_step4 = llm.chat(messages_step4)\n",
    "    raw_llm_output_step4 = output_step4.message.content\n",
    "\n",
    "    # --- Robust JSON Parsing Logic ---\n",
    "    generated_fhir_json = None\n",
    "    try:\n",
    "        json_string = \"\"\n",
    "        # 1. Try to find JSON within markdown fences first\n",
    "        if '```json' in raw_llm_output_step4 and '```' in raw_llm_output_step4:\n",
    "            json_start = raw_llm_output_step4.find('```json') + len('```json')\n",
    "            json_end = raw_llm_output_step4.rfind('```')\n",
    "            json_string = raw_llm_output_step4[json_start:json_end].strip()\n",
    "        else:\n",
    "            # 2. Fallback: find the first '{' and the last '}'\n",
    "            first_brace = raw_llm_output_step4.find('{')\n",
    "            last_brace = raw_llm_output_step4.rfind('}')\n",
    "            if first_brace != -1 and last_brace != -1 and last_brace > first_brace:\n",
    "                json_string = raw_llm_output_step4[first_brace : last_brace + 1].strip()\n",
    "            else:\n",
    "                json_string = raw_llm_output_step4.strip() # Last resort\n",
    "\n",
    "        generated_fhir_json = json.loads(json_string)\n",
    "        final_fhir_json_outputs.append(generated_fhir_json)\n",
    "\n",
    "        #print(f\"  Successfully generated FHIR '{fhir_type_str}' JSON:\")\n",
    "        #pprint(generated_fhir_json)\n",
    "        #print(\"=\" * 80) # Separator for clarity\n",
    "\n",
    "    #except json.JSONDecodeError as e:\n",
    "        #print(f\"  ERROR: Failed to parse LLM's JSON output for '{fhir_type_str}': {e}\")\n",
    "        #print(f\"  Attempted to parse:\\n---\\n{json_string}\\n---\")\n",
    "        #print(f\"  Full raw LLM output:\\n---\\n{raw_llm_output_step4}\\n---\")\n",
    "        #print(\"-\" * 60)\n",
    "    except Exception as e:\n",
    "        print(f\"  An unexpected error occurred for '{fhir_type_str}': {e}\")\n",
    "        print(f\"  Full raw LLM output:\\n---\\n{raw_llm_output_step4}\\n---\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "if not final_fhir_json_outputs:\n",
    "    print(\"\\nNo FHIR resources were successfully generated in Step 4.\")\n",
    "else:\n",
    "    print(\"\\n--- Summary of All Generated FHIR HL7 JSON Outputs ---\")\n",
    "    for idx, fhir_json in enumerate(final_fhir_json_outputs):\n",
    "        print(f\"\\nResource {idx + 1}:\")\n",
    "        pprint(fhir_json)\n",
    "        print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Whole Runthrough",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}